# Tests de l'Orchestrateur

Ce dossier contient les tests unitaires et d'intÃ©gration pour le service d'orchestration.

## ğŸ—‚ Structure

```
tests/
â”œâ”€â”€ conftest.py              # Configuration pytest et fixtures
â”œâ”€â”€ test_config.yaml         # Configuration pour les tests
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ test_workflow_service.py     # Tests du service principal
â”‚   â”œâ”€â”€ test_document_client.py      # Tests du client Document Processor
â”‚   â”œâ”€â”€ test_transaction_client.py   # Tests du client Transaction Analyzer
â”‚   â””â”€â”€ test_supabase_client.py      # Tests du client Supabase
â””â”€â”€ README.md               # Cette documentation
```

## ğŸš€ ExÃ©cution des tests

### Tests unitaires
```bash
# Tous les tests
pytest

# Tests spÃ©cifiques
pytest tests/services/test_workflow_service.py

# Avec couverture
pytest --cov=app tests/

# Avec rapport dÃ©taillÃ©
pytest --cov=app --cov-report=html tests/
```

## ğŸ”§ Configuration

### Fixtures

```python
# Configuration de test
@pytest.fixture
def test_config():
    return ServiceConfig(config_path='tests/test_config.yaml')

# Clients mockÃ©s
@pytest.fixture
def mock_document_client(test_config):
    return Mock(spec=DocumentProcessorClient)

@pytest.fixture
def mock_transaction_client(test_config):
    return Mock(spec=TransactionAnalyzerClient)
```

## ğŸ“ Tests par module

### WorkflowService
- Start workflow
- Traitement des documents
- Analyse des transactions
- Gestion des erreurs
- Retries

### DocumentProcessorClient
- Process document
- Health check
- Gestion des erreurs HTTP
- Retry automatique

### TransactionAnalyzerClient
- Analyse simple et par lots
- Validation des rÃ©sultats
- Health check
- Gestion des timeouts

### SupabaseClient
- CRUD workflows
- Stockage transactions
- Gestion des erreurs
- Row Level Security

## ğŸ›¡ï¸ Mocking

### HTTP Requests
```python
@pytest.mark.asyncio
async def test_api_call(mock_httpx_client):
    mock_response = Mock(status_code=200)
    mock_httpx_client.post = AsyncMock(return_value=mock_response)
```

### Base de donnÃ©es
```python
@pytest.fixture
def mock_supabase():
    with patch('app.services.supabase_client.create_client') as mock:
        yield mock.return_value
```

## ğŸ” Couverture de tests

### Objectifs
- Code mÃ©tier : > 90%
- Utilitaires : > 80%
- Total : > 85%

### Exclusions
- Code de configuration
- Logs et mÃ©triques
- Scripts de migration

## ğŸ¯ Types de tests

### Tests unitaires
- Isolation complÃ¨te (mocks)
- Rapides et dÃ©terministes
- Un cas de test par scÃ©nario

### Tests asynchrones
- Utilisation de pytest-asyncio
- Mocking des appels async
- Gestion des timeouts

## ğŸ› Debugging

### Logs dÃ©taillÃ©s
```bash
pytest -v --log-cli-level=DEBUG
```

### Breakpoints
```python
import pdb; pdb.set_trace()
# ou
import ipdb; ipdb.set_trace()
```

## ğŸ“Š Best Practices

1. **Organisation**
   - Un fichier de test par module
   - Nommage clair et descriptif
   - Documentation des cas complexes

2. **Mocking**
   - Mock uniquement le nÃ©cessaire
   - Utiliser les specs pour la validation
   - Reset des mocks entre les tests

3. **Assertions**
   - VÃ©rifications prÃ©cises
   - Messages d'erreur clairs
   - Validation des effets de bord

4. **Maintenance**
   - Mise Ã  jour avec le code
   - Revue rÃ©guliÃ¨re des tests
   - Nettoyage du code mort

## ğŸ‘· Contribution

1. **Nouveaux tests**
   - Suivre la structure existante
   - Documenter les cas complexes
   - VÃ©rifier la couverture

2. **Modifications**
   - Maintenir les tests existants
   - Ajouter les nouveaux cas
   - Mettre Ã  jour la documentation

## ğŸ”® TODO

- [ ] Ajouter des tests de performances
- [ ] ImplÃ©menter des tests d'intÃ©gration
- [ ] AmÃ©liorer la couverture async
- [ ] Ajouter des tests de charge